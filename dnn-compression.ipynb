{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Compression with the Tensor-Train Decomposition in ``TensorLy`` and ``PyTorch``\n",
    "\n",
    "In this notebook, we will show how to use **TensorLy** [1] to store the weights of a deep neural network (DNN) in the Tensor-Train (TT) format [2].\n",
    "\n",
    "As a toy example, we show how one can use 1/4 of the number of parameters for a network to classify digits in the MNIST dataset, whilst sacrificing only a 2% drop in classification over its counterpart in non TT-format.\n",
    "\n",
    "Included is the following:\n",
    "\n",
    "1. [Storing an FC layer's weights in the TT-format](#Storing-an-FC-layer's-weights-in-the-TT-format)\n",
    "2. [Defining the networks](#Define-the-networks)\n",
    "3. [Model comparison (number of parameters vs accuracy)](#Model-comparison)\n",
    "\n",
    "---\n",
    "\n",
    "[1]: Jean Kossaifi, Yannis Panagakis, Anima Anandkumar and Maja Pantic, TensorLy: Tensor Learning in Python, https://arxiv.org/abs/1610.09555.\n",
    "\n",
    "[2]: A. Novikov, D. Podoprikhin, A. Osokin, and D. Vetrov, ‘Tensorizing Neural Networks’, arXiv:1509.06569 [cs], Dec. 2015, Accessed: Jul. 27, 2020. [Online]. Available: http://arxiv.org/abs/1509.06569."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorly as tl\n",
    "from tensorly.random import check_random_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import TensorLy and set the backend to PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "tl.set_backend('pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters of the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the MNIST dataset. The following code will automatically download the data in a `data` folder if it hasn't already been done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize((32, 32)),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data/', train=False, transform=transforms.Compose([\n",
    "                       transforms.Resize((32, 32)),\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing an FC layer's weights in the TT-format\n",
    "\n",
    "One option is to apply the TT-decomposition directly on the weight matrices $\\mathbf{W}$. This simply corresponds to the standard matrix decomposition however, and thus doesn't fully exploit the parameter-saving capabilities of the TT-decomposition.\n",
    "\n",
    "Given the weights $\\mathbf{W}$ of a fully-connected layer, we first reshape the matrix into a higher-order tensor $\\mathcal{W}$. We can then represent $\\mathcal{W}$ in the TT-format--greatly reducing the number of parameters we have to store, with us only needing to store the TT-cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dimensions(rows, cols):\n",
    "    # form the dims of the tensor W: # kth dimension is formed by i_k*j_k\n",
    "    return [I_k*J_k for I_k, J_k in zip(rows, cols)]\n",
    "\n",
    "class TTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Stores the weights of a fully connected layer in the TT-format\n",
    "    \"\"\"\n",
    "    def __init__(self, matrix_shape, cols, rows, rank=8, **kwargs):\n",
    "        super(TTLayer, self).__init__(**kwargs)\n",
    "        \n",
    "        assert(matrix_shape[0]==np.prod(cols))\n",
    "        assert(matrix_shape[1]==np.prod(rows))\n",
    "        \n",
    "        self.rank = rank\n",
    "        self.matrix_shape = matrix_shape\n",
    "        \n",
    "        # get the dimensions of the len(rows)-order tensor\n",
    "        self.dimensions = get_dimensions(rows, cols)\n",
    "        \n",
    "        # Add and register the factors\n",
    "        self.factors = nn.ParameterList(\n",
    "            [nn.Parameter(f, requires_grad=True) for f in tl.random.random_mps(self.dimensions, rank)]\n",
    "        )\n",
    "        \n",
    "        for f in self.factors:\n",
    "            f.data.uniform_(-0.1, 0.1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # form full tensor\n",
    "        W = tl.mps_to_tensor(self.factors)\n",
    "        W = tl.reshape(W, self.matrix_shape)\n",
    "        \n",
    "        return torch.matmul(W, x)\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        return sum([p.numel() for p in self.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the networks\n",
    "\n",
    "We'll define a very simple fully-connected network with two layers, following the experiments section in [2]. The network is thus composed of two weight matrices: $\\mathbf{W}^{(1)}\\in\\mathbb{R}^{1024\\times1024}$ and $\\mathbf{W}^{(2)}\\in\\mathbb{R}^{10\\times1024}$.\n",
    "\n",
    "![simple architecture](./images/fc-tt.png)\n",
    "\n",
    "We'll store these weights explicitly in the first model, and then we'll also store them in the TT-format in the second two models, and see how the networks perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, use_tt=False, rank=32):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # w1 (1024 x 1024)\n",
    "        # desired TT-shapes for width and height of matrix\n",
    "        w1_cols = [4, 4, 4, 4, 4]\n",
    "        w1_rows = [4, 4, 4, 4, 4]\n",
    "\n",
    "        # w2 (10 x 1024)\n",
    "        w2_cols = [5, 2, 1, 1, 1]\n",
    "        w2_rows = [4, 4, 4, 4, 4]\n",
    "        \n",
    "        self.fc1 = TTLayer((1024, 1024), w1_cols, w1_rows, rank=rank) if use_tt else nn.Linear(1024, 1024)\n",
    "        self.fc2 = TTLayer((10, 1024), w2_cols, w2_rows, rank=rank) if use_tt else nn.Linear(1024, 10)\n",
    "        \n",
    "    def get_num_params(self):\n",
    "        return sum([p.numel() for p in self.parameters() if p.requires_grad])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.Flatten()(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "    \n",
    "ranks = [64, 32]\n",
    "models = [\n",
    "    Net(use_tt=False),\n",
    "    Net(use_tt=True, rank=ranks[0]),\n",
    "    Net(use_tt=True, rank=ranks[1]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network 0...\n",
      "Test set acc: 97.9000015258789%\tTime taken: 436.562253482\n",
      "Training network 1...\n",
      "Test set acc: 96.02999877929688%\tTime taken: 2090.609674247\n",
      "Training network 2...\n",
      "Test set acc: 95.02999877929688%\tTime taken: 1030.9063246240003\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epoch = 5 # Number of epochs\n",
    "\n",
    "models_acc = []\n",
    "for i, model in enumerate(models):\n",
    "    model = model.to(device)\n",
    "    print('Training network {}...'.format(i))\n",
    "    start = time.process_time()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(n_epoch):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Important: do not forget to reset the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(data)\n",
    "            loss = criterion(output,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    def test():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss = criterion(output,target)\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        \n",
    "        return 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    for epoch in range(1, n_epoch):\n",
    "        train(epoch)\n",
    "        acc = test()\n",
    "        \n",
    "    print('Test set acc: {}%\\tTime taken: {}'.format(acc, time.process_time() - start))\n",
    "    models_acc += [acc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "\n",
    "Plotted below is the models' number of parameters and their respective test-set accuracies after 5 epochs of training.\n",
    "\n",
    "As we can see, storing the weights in the TT-format allows us to store the model with very few paramters: even as few as 1/25 the number of parameters of the original network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dfXhV5Z3u8e9NxEYFteXFCoixPVpj0eaEgDpUirUq5XRkoK2jA8O0VZkKztG+0bHnasd6Znxh6ohXR22laKcyVntqrVYsylRFZSoQFBDFgrXpGOkIoiKIFIK/88daO92ElbAjWdmbcH+uK1ey3n87sPed9TxrPUsRgZmZWVu9yl2AmZlVJgeEmZllckCYmVkmB4SZmWVyQJiZWSYHhJmZZTogz51LuhS4CBAwOyJmSaoDvgdUAy3AtIhYkrHtWOAGoAr4QURcs6fj9e/fP2pqarrwFZiZ9WzLli17NSIGZC3LLSAkDSMJh5HAdmC+pHnATODbEfFLSePS6TFttq0CbgTOBJqBpZLui4jnOjpmTU0NjY2NXf5azMx6Kkm/b29Znk1MtcCTEbE1IlqAhcAEIIBD03UOA9ZlbDsSeCEiXoyI7cCdwPgcazUzszbybGJaBfyTpH7A28A4oBG4DHhQ0ndIAurPMrYdDLxUNN0MnJxjrWZm1kZuZxARsRq4FlgAzAdWkPQ5XAx8KSKOAr4EzMnYXFm7zDqOpKmSGiU1btiwoUtqNzOznDupI2IOaQBIuorkTOBq4NJ0lf8H/CBj02bgqKLpIWQ3RRERtwC3ADQ0NOwWIjt27KC5uZlt27a9y1dhe1JdXc2QIUPo3bt3uUsxsy6U91VMAyNivaShwETgVODvgI8BjwIfB9ZmbLoUOFbSMcDLwHnAX72bGpqbm+nbty81NTVIWScmtjcigo0bN9Lc3MwxxxxT7nLM9isRscvnWtvpvZVrQAB3p30QO4DpEfG6pIuAGyQdAGwDpgJIGkRyOeu4iGiRdAnwIMllrrdGxLPvpoBt27Y5HHIkiX79+uHmPbPudf2CNby5bQff+tQJSCIiuPL+5zi0ujdfOvO4LjlG3k1Mp2XMewIYnjF/HUlHdmH6AeCBrqjD4ZAv/37NuldE8Oa2Hdy2qAmAb33qBK68/zluW9TE50fVdNmZRN5nEAb06dOHLVu27DLve9/7HgcffDBTpkwpU1Vmtq+SxLc+dQIAty1qag2Kz4+qaT2j6AoOiDbybtMr+OIXv9jl+ywWEUQEvXp5NBWznqgQEoVwALo0HMBjMe3i+gVruPL+5yg8Za/Qpnf9gjVdfqwrrriC73znOwCMGTOGr3/964wcOZLjjjuOxx9/HICdO3fyta99jREjRnDSSSfx/e9/H4AtW7ZwxhlnUF9fz4knnsi9994LQFNTE7W1tUybNo36+npeeuml7IOb2T6v8PlUrPjzqys4IFLFbXqFX3KhTe/NbTu69JeepaWlhSVLljBr1iy+/e1vAzBnzhwOO+wwli5dytKlS5k9eza/+93vqK6u5p577uGpp57ikUce4Stf+Uprfb/5zW+YMmUKTz/9NEcffXSuNZtZeRR/Pn1+VA2/u3ocnx9Vs8vnV1dwE1Oqu9r02jNx4kQAhg8fTlNTcuyHHnqIlStX8tOf/hSATZs2sXbtWoYMGcI3vvENHnvsMXr16sXLL7/MK6+8AsDRRx/NKaeckmutZlZekji0uvcun0+Fz69Dq3u7DyIP3dGm1573vOc9AFRVVdHS0gIkfyV897vf5eyzz95l3R/+8Ids2LCBZcuW0bt3b2pqalpvBDzkkENyr9XMyu9LZx63Sx9p4fPLfRA56Y42vc44++yzufnmm9mxYwcAa9as4a233mLTpk0MHDiQ3r1788gjj/D737c7GKOZ9WBtw6Cr/5j1GUSqbZte8XXFsHdnElu3bmXIkCGt01/+8pdL2u7CCy+kqamJ+vp6IoIBAwbw85//nEmTJvHnf/7nNDQ0UFdXx/HHH/+u6jIz64jK9ddxHhoaGqLt8yBWr15NbW1tSdt3x52JPVVnfs9mVjkkLYuIhqxlPoMo0h1temZm+wr3QbSRd5uemdm+wgFhZmaZHBBmZpbJAWFmZpkcEGZmlslXMeVo48aNnHHGGQD893//N1VVVbzvfe+jqqqK7du389prr1FVVcWAAQMAWLJkCQceeGCX1lBTU0NjYyP9+/fvcL1HH32Uyy67jB07dtC/f38WLlzYumznzp00NDQwePBg7r///i6tz8wqlwOirQgovnKp7XQn9OvXj+XLlwPJ6K19+vThq1/9auvyrHntaWlp4YAD8vnneuONN5g2bRrz589n6NChrF+/fpflN9xwA7W1tbz55pu5HN/MKpObmIo9cjXMvzwJBUi+z788mV8GV1xxBVOnTuWss85iypQpNDU1cdppp1FfX099fT3/+Z//CSR//Y8ZM4bPfOYzHH/88UyaNGm34UHefvttxo4dy+zZs3c7zh133MHEiRMZOnQoAAMHDmxd1tzczLx587jwwgtzfKVmVol8BlEQAds2weKbk+mxaVgsvhlOvnivziT2xrJly3jiiSc46KCD2Lp1KwsWLKC6upq1a9dy/vnnU7hz/Omnn+bZZ59l0KBBjBo1ikWLFvHRj34USJ4fcd555zFlypTMJ9itWbOGHTt2MGbMGDZv3syll17aut5ll13GzJkz2bx5c/e9aDOrCA6IAikJBUhCoRAUJ1+czC/TDXPnnHMOBx10EAA7duzgkksuYfny5VRVVbFmzZ8eZDRy5MjW8Z7q6upoampqDYjx48czY8YMJk2alHmMlpYWli1bxq9+9SvefvttTj31VE455RTWrFnDwIEDGT58OI8++mi+L9TMKo6bmIoVh0RBN4bDjTfeSF1dHXV1daxbtw7Ydfju66+/niOOOIIVK1bQ2NjI9u3bW5cVhguHXYcMBxg1ahS//OUvW5ud2h5nyJAhjB07lkMOOYT+/fszevRoVqxYwaJFi7jvvvuoqanhvPPO4+GHH2by5Ml5/xrMrEI4IIoV+hyKFfdJ5Gz69OksX76c5cuXM2jQoN2Wb9q0iSOPPJJevXpx++23s3PnzpL2e+WVV9KvXz+mTZuWeZzx48fz+OOP09LSwtatW1m8eDG1tbVcffXVNDc309TUxJ133snHP/5x5s6d26Wv2cwqlwOioBAOhT6Hf3gj+b745m4NiY5MmzaNf/u3f2tt/unMw4FmzZrFtm3bmDFjxm7LamtrGTt2LCeddBIjR47kwgsvZNiwYV1Zupntgzzcd7FHrk46qgvNSoXQqD4MTr98z9vvxzzct9m+ycN9l+r0y3e9WqnQJ+ERXc1sP5RrQEi6FLgIEDA7ImZJugv4ULrK4cAbEVGXsW0TsBnYCbS0l3A5FN3xtJnZfiK3gJA0jCQcRgLbgfmS5kXEXxatcx2wqYPdnB4Rr+ZVo5mZtS/PTupa4MmI2BoRLcBCYEJhoZIn8ZwL/DjHGszM7F3KMyBWAaMl9ZN0MDAOOKpo+WnAKxGxtp3tA3hI0jJJU3Os08zMMuTWxBQRqyVdCywAtgArgJaiVc6n47OHURGxTtJAYIGk5yPisbYrpeExFWgdS8jMzPZervdBRMSciKiPiNHAa8BaAEkHABOBuzrYdl36fT1wD0lfRtZ6t0REQ0Q0FIbNrhQbN25svWP5/e9/P4MHD+bEE0+krq6OE044oXVeYZ3iO6O7Sk1NDa++2nE3zr333stJJ51EXV0dDQ0NPPHEEwC89NJLnH766dTW1vLhD3+YG264ocvrM7PKlfdVTAMjYr2koSSBcGq66BPA8xHR3M52hwC9ImJz+vNZwJV51loQEajoyqW2052xrwz3fcYZZ3DOOecgiZUrV3Luuefy/PPPc8ABB3DddddRX1/P5s2bGT58OGeeeSYnnHBCLnWYWWXJ+07quyU9B/wCmB4Rr6fzz6NN85KkQZIeSCePAJ6QtAJYAsyLiPk518pNy29i5tKZrWMWRQQzl87kpuU35X3oTN013HefPn1aQ/Ctt95q/fnII4+kvr4egL59+1JbW8vLL7+c50s2swqS6xlERJzWzvzPZcxbR9KRTUS8CHwkz9oyjs/m7ZuZuzoZa2jGiBnMXDqTuavnMrl28l6dSeyN7hjuG+Cee+7h8ssvZ/369cybN2+35U1NTTz99NOcfPLJ+b1YM6sovpM6JYkZI5JxiuauntsaFJNrJzNjxIyyhAN0z3DfABMmTGDChAk89thjfPOb3+Q//uM/Wpdt2bKFT3/608yaNYtDDz00j5dpZhXIg/UVKQ6Jgu4Mh3IN911s9OjR/Pa3v23t2N6xYwef/vSnmTRpEhMnTuz6F21mFcsBUaTQ51CsuE8ib+Ua7vuFF15ofY1PPfUU27dvp1+/fkQEF1xwAbW1tXz5y1/uuhdqZvsEB0SqEA6FPoeVU1YyuXYyc1fP7daQ6Ehew33ffffdDBs2jLq6OqZPn85dd92FJBYtWsTtt9/Oww8/3HrG8cADD2Ts3cx6Ig/3XeSm5Texefvm1malQmj0PbAv0+qm5VFyj+Hhvs32TR7uu0TT6qbtcrVSoU+iXB3UZmbl5CamNtqGgcPBzPZXDggzM8u0XwRET+pnqUT+/Zr1TD0+IKqrq9m4caM/xHISEWzcuJHq6upyl2JmXazHd1IPGTKE5uZmNmzYUO5Seqzq6urWu7jNrOfo8QHRu3dvjjnmmHKXYWa2z+nxTUxmZvbuOCDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMnU4FpOkEcBk4DTgSOBtYBUwD7gjIjbnXqGZmZVFuwEh6X5gI3AvcB2wHqgGjgNOB+ZJmhkR93dHoWZm1r06OoO4ICJeaTNvG7Ak/bpW0sCOdi7pUuAiQMDsiJgl6S7gQ+kqhwNvRERdxrZjgRuAKuAHEXFNKS/IzMy6RrsBkREOSPoYcDCwICJaImJ9e9tLGkYSDiOB7cB8SfMi4i+L1rkO2JSxbRVwI3Am0AwslXRfRDxX8iszM7O9UnIntaR/Bj4FjAF+XsImtcCTEbE1IlqAhcCEov0JOBf4cca2I4EXIuLFiNgO3AmML7VWMzPbe+0GhKRrJPUtmnV0RHwtIr4O1JSw71XAaEn9JB0MjAOOKlp+GvBKRKzN2HYw8FLRdHM6L6vOqZIaJTX6qXFmZl2nozOIXwI/lXSxpF7Av0taImkFMGdPO46I1cC1wAJgPrACaCla5Xyyzx4g6bPYbZftHOeWiGiIiIYBAwbsqSwzMytRuwEREQuBsSQd0w8C2yNiZER8JCKuL2XnETEnIuojYjTwGrAWQNIBwETgrnY2bWbXs40hwLpSjmlmZl2joyamKuAs4PckH+anSrpH0odL3XnhKidJQ9N9FM4YPgE8HxHN7Wy6FDhW0jGSDgTOA+4r9bhmZrb3OrrM9R7gOZKrliZHxBckDQH+UdK2iPhiCfu/W1I/YAcwPSJeT+efR5vmJUmDSC5nHRcRLZIuITlzqQJujYhnO/fSzMxsbygis2kfSc9ExImSegOLI6K+aFlDRDR2V5GlamhoiMbGiivLzKxiSVoWEQ1Zyzo6g7hV0vL05136HCoxHMzMrGt1dKPc9bQJBjMz23901En995IO7WD5aEnj8inLzMzKraMmprXAQ5LeBJYBG0gG6zsWGE5yZ/Q/5l6hmZmVRUdNTHeTXIVUC4ziT8N9/xS4JCLe6p4SzcysHDp8HgS03hG9uhtqMTOzCuInypmZWSYHhJmZZdpjQEg6vDsKMTOzylLKGcQyST+WdFbu1ZiZWcUoJSCOBX4EXCRpraQrJX0w57rMzKzM9hgQEfFORPwyIj5L8gjRC4Dlkn4laWTuFZqZWVns8TLXtA9iEjAFeB34EslIr8NJnudwTJ4FmplZeewxIEiezXAHcG5E/L5o/pOSZudTlpmZlVspAfGhiHgna0FEXNXF9ZiZWYUopZP6geJLXSW9V9K8HGsyM7MKUEpAvD8i3ihMpE+FG5RfSWZmVglKCYid6aNGgdbnS5uZWQ9XSh/Et4BFkh5Op08HLs6vJDMzqwSljOY6L73f4VRAwNcjYn3ulZmZWVmVOljfNuC/gFeA/yHpz/IryczMKkEpN8p9AfgKMBh4BhgBPAmMybUyMzMrq1LOIL4ENABNEXEayR3Uf8i1KjMzK7tSAmJbRLwNIOnAiHgWOD7fsszMrNxKuYrpD+mNcr8AHpT0GklfhJmZ9WClXMV0TvrjNyWdARwGlHQntaRLSUaAFTA7Imal8/8OuARoAeZFxIyMbZuAzcBOoCUiGko5ppmZdY0OA0JSFfBURHwEICJ+VeqOJQ0jCYeRwHZgfjpExxBgPHBSRPxR0sAOdnN6RLxa6jHNzKzrdBgQEbFT0nOSBkfEy53cdy3wZERsBZC0EJhA0uF9TUT8MT2G76kwM6tApXRS9wdWS3pQ0s8KXyVstwoYLamfpIOBccBRwHHAaZIWS1ooaUQ72wfwkKRlkqa2dxBJUyU1SmrcsGFDCWWZmVkpSumkvubd7DgiVku6FlgAbAFWkPQ5HAC8FziF5J6Kn0j6QEREm12Mioh1aRPUAknPR8RjGce5BbgFoKGhoe0+zMzsXSqlk7rkfoeMbecAcwAkXQU0kzQ9/SwNhCWS3iE5S9nQZtt16ff1ku4h6cvYLSDMzCwfe2xikrRZ0pvp11ZJf5T0Zik7L3RApyPATgR+DPwc+Hg6/zjgQODVNtsdIqlv4WfgLJImKzMz6yalnEH0LfwsqRfJB/1HStz/3ZL6ATuA6RHxuqRbgVslrSK5uulvIiIkDQJ+EBHjgCOAeyQVarwjIuZ35oWZmdne0e5N/yVsJD0ZEafkUM9eaWhoiMbGxnKXYWa2z5C0rL37zEoZrO+cosleJJepqotqMzOzClXKVUyfLfq5BWgiudHNzMx6sFL6IP66OwoxM7PKUspVTHPSwfoK0++VNDvfsszMrNxKuZO6PiLeKExExOskz4QwM7MerJSA6CXpsMKEpPcCvfMryczMKkEpndSzgF9LuotkfKTzgJm5VmVmZmVXSif1bZKWkdz9LOAvI+KZ3CszM7OyKuU+iBHA6ohYmU73ldQQEb4jzcysByulD+IWYGvR9FvA9/Mpx8zMKkVJndQR8U5hIv3ZndRmZj1cKQHxO0kXS6qS1EvSdJK7qc3MrAcrJSD+FjgDeCX9+hjJs6bNzKwHK+UqpleAz3RDLWZmVkFKuYrpPcDngA8D1YX5EdHuc6LNzGzfV0oT04+AGuBTwGLgg8C2HGsyM7MKUEpAHBcRlwNb0mdMjwWG5VuWmZmVWykBsSP9/oakWqAvcHR+JZmZWSUoZSymOekAff8APAgcDHwr16rMzKzsSrmKqXDX9CPA0HzLMTOzSlFKE5OZme2HHBBmZpaplEeO7tYMlTXPzDohouNpswpQyhnEkhLnmVkpHrka5l/+p1CISKYfubq8dZm10e6ZgKSBwJHAQZJOJHlYEMChJFcymVlnRcC2TbD45mR6bBoWi2+Gky9Olksd78Osm3TUVPS/gC8AQ4Ab+VNAbAa+WcrOJV1KMrCfgNkRMSud/3fAJUALMC8iZmRsOxa4AagCfhAR15RyTLOKJiWhAEkoFILi5IuT+Q4HqyDtBkRE3AbcJunciPhJZ3csaRhJOIwEtgPzJc0jCZzxwEkR8cf0TKXttlUkoXQm0AwslXRfRDzX2TrMKk4hJArhAA4Hq0il9EEMlHQogKTvSVoi6YwStqsFnoyIrRHRAiwEJgAXA9dExB8BImJ9xrYjgRci4sWI2A7cSRIqZvu+Qp9DseI+CbMKUUpATI2INyWdRfLX/8XAzBK2WwWMltRP0sHAOOAo4DjgNEmLJS1Mn3nd1mDgpaLp5nTebiRNldQoqXHDhg0llGVWRoVwKPQ5/MMbyffFNzskrOKUcrlq4X/sJ4HbImKZpD0GS0SslnQtsADYAqwg6XM4AHgvcAowAviJpA9E7PLOyDrXznznRMQtJM/NpqGhwe8uq2wSVB+2a59DoU+i+jA3M1lFKSUgVkh6gOQv//8jqQ/tfFi3lY7+OgdA0lUkZwK1wM/SQFgi6R2gP1D8538zydlGwRBgXSnHNKt4p1++69VKhZBwOFiFKSUgPg8MJ+kT2CqpP3BBKTuXNDAi1ksaCkwETgXeAT4OPCrpOOBA4NU2my4FjpV0DPAycB7wV6Uc02yf0DYMHA5WgUppKtoJfICk7wHgoFK2S90t6TngF8D0iHgduBX4gKRVJJ3PfxMRIWlQeqZC2ql9CcnosauBn0TEs514XWZmtpcUe+gUk/SvQG9gdETUSnof8GBEZHUul1VDQ0M0NjaWuwwzs32GpGUR0ZC1rJQmpj+LiHpJTwNExGuSDuzSCs3MrOKU9ES59KqlAJDUj6QfwczMerB2A6JoxNYbgbuBAZK+DTwBXNsNtZmZWRl11MS0BKiPiB9JWgZ8guT+hM9GxKpuqc7MzMqmo4Bove4uvYLIVxGZme1HOgqIAZK+3N7CiPiXHOoxM7MK0VFAVAF9yB72wszMeriOAuIPEXFlt1ViZmYVpaPLXH3mYGa2H+soIEp55oOZmfVQ7QZERLzWnYWYmVllKXXQPTMz2884IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLJMDwszMMjkgzMwskwPCzMwyOSDMzCyTA8LMzDI5IMzMLFOuASHpUkmrJD0r6bJ03hWSXpa0PP0a1862TZKeSddpzLNOMzPbXUePHN0rkoYBFwEjge3AfEnz0sXXR8R3StjN6RHxal41mplZ+3ILCKAWeDIitgJIWghMyPF4ZmbWhfJsYloFjJbUT9LBwDjgqHTZJZJWSrpV0nvb2T6AhyQtkzS1vYNImiqpUVLjhg0buvYVmJntx3ILiIhYDVwLLADmAyuAFuBm4INAHfAH4Lp2djEqIuqBTwLTJY1u5zi3RERDRDQMGDCgi1+Fmdn+K9dO6oiYExH1ETEaeA1YGxGvRMTOiHgHmE3SR5G17br0+3rgnvbWMzOzfOR9FdPA9PtQYCLwY0lHFq0ygaQpqu12h0jqW/gZOCtrPTMzy0+endQAd0vqB+wApkfE65Jul1RH0sfQBPwtgKRBwA8iYhxwBHCPpEKNd0TE/JxrNTOzIrkGREScljHvr9tZdx1JRzYR8SLwkTxrMzOzjvlOajMzy+SAMDOzTA4IMzPL5IAwM7NMDggzM8vkgDAzs0wOCDMzy+SAMDOzTA4IMzPL5IAwM7NMDggzM8vkgDAzs0wOCDMzy+SAMDOzTA4IMzPL5IAwM7NMDggzM8vkgDAzs0wOCDMzy+SAMDOzTA4IMzPL5IAwM7NMDoh3ISI6nDYz6wkcEJ100/KbmLl0ZmsoRAQzl87kpuU3lbkyM7Ou5YDohIhg8/bNzF09tzUkZi6dydzVc9m8fbPPJMysRzkgz51LuhS4CBAwOyJmSboinbchXe0bEfFAxrZjgRuAKuAHEXFNnrWWQhIzRswAYO7qucxdPReAybWTmTFiBpLKWZ6ZWZfK7QxC0jCSIBgJfAT4lKRj08XXR0Rd+pUVDlXAjcAngROA8yWdkFetnVEcEgUOBzPrifJsYqoFnoyIrRHRAiwEJpS47UjghYh4MSK2A3cC43Oqs1MKzUrFivskzMx6ijwDYhUwWlI/SQcD44Cj0mWXSFop6VZJ783YdjDwUtF0czpvN5KmSmqU1Lhhw4asVbpMcZ/D5NrJrJyyksm1k3fpkzAz6ylyC4iIWA1cCywA5gMrgBbgZuCDQB3wB+C6jM2z2msyP30j4paIaIiIhgEDBnRF6e2SRN8D++7S5zBjxAwm106m74F93cxkZj1Krp3UETEHmAMg6SqgOSJeKSyXNBu4P2PTZv50tgEwBFiXY6klm1Y3jYhoDYNCSDgczKynyfUyV0kD0+9DgYnAjyUdWbTKBJKmqLaWAsdKOkbSgcB5wH151toZbcPA4WBmPVGuZxDA3ZL6ATuA6RHxuqTbJdWRNBk1AX8LIGkQyeWs4yKiRdIlwIMkl7neGhHP5lyrmZkVybuJ6bSMeX/dzrrrSDqyC9MPALtdAmtmZt3Dd1KbmVkmB4SZmWVyQJiZWSYHhJmZZVJPuvtX0gbg93uxi8OATWXY/t1s15lt+gOvdnL/+5u9/bfvTuWqNe/jduX+/V4u3dERkX2XcUT4K/0CbinH9u9mu85sAzSW+3db6V97+2+/P9Sa93G7cv9+L3fNl5uYdvWLMm3/brbb21ptV/vS77NcteZ93K7cv9/LXaBHNTFZNkmNEdFQ7jrMbO9093vZZxD7h1vKXYCZdYlufS/7DMLMzDL5DMLMzDI5IMzMLJMDwszMMjkg9jOSxkh6XNL3JI0pdz1m9u5J6iXpnyR9V9LfdPX+HRA9QPps7/WSVrWZP1bSbyS9IOnv09kBbAGqSZ7cZ2YVpJPv5/HAYJJn7nT5+9lXMfUAkkaTfOj/KCKGpfOqgDXAmST/cZYC5wPPR8Q7ko4A/iUiJpWpbDPL0Mn38znA6xHxfUk/jYjPdGUtPoPoASLiMeC1NrNHAi9ExIsRsR24ExgfEe+ky18H3tONZZpZCTrzfiYJi9fTdXZ2dS15P3LUymcw8FLRdDNwsqSJwNnA4cC/lqMwM+u0zPczcAPwXUmnAY919UEdED2XMuZFRPwM+Fl3F2Nme6W99/NW4IK8Duompp6rGTiqaHoIsK5MtZjZ3inL+9kB0XMtBY6VdIykA4HzgPvKXJOZvTtleT87IHoAST8Gfg18SFKzpAsiogW4BHgQWA38JCKeLWedZrZnlfR+9mWuZmaWyWcQZmaWyQFhZmaZHBBmZpbJAWFmZpkcEGZmlskBYWZmmRwQVvEkXZ0+x+IvioY5zvN4n5PU4ThVpaxTSSQdLmlaueuwfYsDwvYFJwOLgY8Bj5e5ltwokdd78nCgUwGRcz22D/A/vlUsSf8saSUwguTO0guBmyV9K2PdH0q6WdIjkl6U9LH0wSurJf2waL3zJT0jaZWka4vmf17SGkkLgd9pJUUAAAN8SURBVFFF8wdIulvS0vRrFB2QdIWk2yU9LGmtpIvS+X0k/UrSU+nxx6fza9IabwKeAo5KX0ejpGclfbto302SrpL063R5vaQHJf1W0heL1vtaWuvKou2vAT4oabmkf25vvXbq+WH6+3pG0pdK+bezHiIi/OWviv0iGQf/u0BvYFEH6/2QZIx8kYyT/yZwIskfQcuAOmAQ8F/AAJKRjB8G/gI4smj+gcAi4F/T/d4BfDT9eSiwOv35c4V12tRxBbACOAjoTzJE86D0eIem6/QHXkhrrQHeAU4p2sf70u9VwKPASel0E3Bx+vP1wEqgb1r3+nT+WcAt6b57AfcDo9PjrCo6RkfrtdYDDAcWFG13eLn/T/ir+7483LdVuv8JLAeOB57bw7q/iIiQ9AzwSkQ8AyDpWZIPvqOBRyNiQzr/30k+FGkz/y7guHT+J4ATpNbRlg+V1HcPddwbEW8Db0t6hCTk5gFXpU8Le4dkfP8j0vV/HxFPFm1/rqSpJKFyJHACSRjAnwZoewboExGbgc2Stkk6nOSD/yzg6XS9PsCxJAFYrKP1iut5EfiApO+mr+GhPbx260EcEFaRJNWRnBUMAV4FDk5mazlwavoB3NYf0+/vFP1cmD4AaOngkO0NStYr63hFgVHKvgKYRPKX/vCI2CGpieS54ABvFe33GOCrwIiIeD1tHqsu2teeXqOAqyPi+23qrWlTU0frtdaT1vARkodMTQfOBb6Q+aqtx3EfhFWkiFgeEXUkz+E9gaQ56OyIqGsnHEqxGPiYpP7pM37PBxam88dI6iepN/DZom0eIhlFE2gNrj0ZL6laUj9gDMlQzYeRNAPtkHQ6ydlMlkNJPqA3KXlu+Cc79QqT0T6/IKlPWu9gSQOBzSTNUXtabxeS+gO9IuJu4JtAfSfrsX2YzyCsYkkaQPJA9nckHR8Re2pi6lBE/EHS5cAjJH9BPxAR96bHuoKkI/wPJJ2zVelm/xu4Me0sP4DksY5fpGNLSJpjhgL/NyLWpc1Zv5DUSNJk9nw7Na6Q9DTwLEnzzqJOvsaHJNUCv07PcrYAkyPit5IWSVoF/DIivpa1Hrs/13gwcFvR1UyXd6Ye27d5uG+zLpQGzZaI+E65azHbW25iMjOzTD6DMDOzTD6DMDOzTA4IMzPL5IAwM7NMDggzM8vkgDAzs0wOCDMzy/T/AXenxe0QeLfjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "xs = ['Linear'] + ['TT-rank-{}'.format(rank) for rank in ranks]\n",
    "for i, x in enumerate(xs):\n",
    "    plt.scatter(models[i].get_num_params(), models_acc[i], label=x, marker='x')\n",
    "    plt.legend()\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlabel('# model parameters')\n",
    "plt.ylabel('Test accuracy (%)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
